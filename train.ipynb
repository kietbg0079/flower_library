{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1B36Jz09o9ui1Ht5ThPnO61_0-z_95xaG",
      "authorship_tag": "ABX9TyP/XzFDC1vjcgV+iUs/3teH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kietbg0079/flower_library/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHJ_YgzBVSoD"
      },
      "source": [
        "import zipfile as zp"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M8zRO_3Wz85"
      },
      "source": [
        "path = '/content/drive/MyDrive/AIL201/AIP302/flower_data.zip'\n",
        "\n",
        "raw_data = zp.ZipFile(path)\n",
        "raw_data.extractall('/content/')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZxowz5TXCyt"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil \n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import models,layers,optimizers\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense, BatchNormalization\n",
        "\n",
        "#from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping,  ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.resnet import ResNet101, preprocess_input"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnrtKyzzMYwB"
      },
      "source": [
        "batch_size = 64\n",
        "img_height = 224\n",
        "img_width = 224"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzo61y2RXw5x",
        "outputId": "686ee2c0-6815-4b08-bbe4-e0c46296e703"
      },
      "source": [
        "TRAINING_DIR = '/content/flower_data/train'\n",
        "\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                   rotation_range=30,\n",
        "                                   zoom_range=0.4,\n",
        "                                   horizontal_flip=True,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   )\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(img_height, img_width))\n",
        "\n",
        "\n",
        "VALIDATION_DIR = '/content/flower_data/valid'\n",
        "\n",
        "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              class_mode='categorical',\n",
        "                                                              target_size=(img_height, img_width))  "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6552 images belonging to 102 classes.\n",
            "Found 818 images belonging to 102 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOBkpYudYioc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bd2dd35-608b-4a55-c2fe-e87ffaba7cc9"
      },
      "source": [
        "adam = Adam(learning_rate=3e-4)\n",
        "\n",
        "resnet101_base = ResNet101(include_top=True, weights='imagenet',\n",
        "                          input_shape=(img_width, img_height,3))\n",
        "\n",
        "output = resnet101_base.get_layer(index = -1).output  \n",
        "output = Flatten()(output)\n",
        "\n",
        "output = Dense(512,activation = \"relu\")(output)\n",
        "output = BatchNormalization()(output)\n",
        "output = Dropout(0.2)(output)\n",
        "output = Dense(512,activation = \"relu\")(output)\n",
        "output = BatchNormalization()(output)\n",
        "output = Dropout(0.2)(output)\n",
        "output = Dense(102, activation='softmax')(output)\n",
        "\n",
        "resnet101_model = Model(resnet101_base.input, output)\n",
        "for layer in resnet101_model.layers[:-7]:\n",
        "    layer.trainable = False\n",
        "resnet101_model.summary()\n",
        "\n",
        "resnet101_model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics =['accuracy'])"
      ],
      "execution_count": 7,
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkwzso-x80KX"
      },
      "source": [
        "resnet101_model.load_weights('/content/drive/MyDrive/AIL201/AIP302/flower_weight/flower_weight.h5')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UdHfYWkYj9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f704d79e-7053-4d22-ae8c-69bad5b50e33"
      },
      "source": [
        "epoch = 10\n",
        "\n",
        "history = resnet101_model.fit(train_generator,\n",
        "                              epochs=epoch,\n",
        "                              verbose=1,\n",
        "                              validation_data=validation_generator,\n",
        "                              callbacks = [ModelCheckpoint('Landmark_weights_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
        "                                                          monitor='val_loss',\n",
        "                                                          verbose=1,\n",
        "                                                          save_best_only=True,\n",
        "                                                          save_weights_only=True,\n",
        "                                                          mode='auto',\n",
        "                                                          period=1),\n",
        "                                          EarlyStopping(monitor='val_loss',\n",
        "                                                        patience=7, \n",
        "                                                        verbose=1, \n",
        "                                                        min_delta=0.001),\n",
        "                              ])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/10\n",
            "103/103 [==============================] - 180s 1s/step - loss: 1.7780 - accuracy: 0.5215 - val_loss: 2.3261 - val_accuracy: 0.4071\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.32609, saving model to Landmark_weights_epoch-01_loss-1.7780_val_loss-2.3261.h5\n",
            "Epoch 2/10\n",
            "103/103 [==============================] - 140s 1s/step - loss: 1.7566 - accuracy: 0.5275 - val_loss: 2.1429 - val_accuracy: 0.4731\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.32609 to 2.14291, saving model to Landmark_weights_epoch-02_loss-1.7566_val_loss-2.1429.h5\n",
            "Epoch 3/10\n",
            "103/103 [==============================] - 140s 1s/step - loss: 1.7445 - accuracy: 0.5256 - val_loss: 2.4669 - val_accuracy: 0.4144\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 2.14291\n",
            "Epoch 4/10\n",
            "103/103 [==============================] - 141s 1s/step - loss: 1.7707 - accuracy: 0.5211 - val_loss: 2.2799 - val_accuracy: 0.4267\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 2.14291\n",
            "Epoch 5/10\n",
            "103/103 [==============================] - 141s 1s/step - loss: 1.7577 - accuracy: 0.5198 - val_loss: 2.3287 - val_accuracy: 0.4572\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 2.14291\n",
            "Epoch 6/10\n",
            "103/103 [==============================] - 139s 1s/step - loss: 1.7614 - accuracy: 0.5201 - val_loss: 2.6536 - val_accuracy: 0.4046\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 2.14291\n",
            "Epoch 7/10\n",
            "103/103 [==============================] - 138s 1s/step - loss: 1.7457 - accuracy: 0.5246 - val_loss: 2.1241 - val_accuracy: 0.4645\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.14291 to 2.12414, saving model to Landmark_weights_epoch-07_loss-1.7457_val_loss-2.1241.h5\n",
            "Epoch 8/10\n",
            "103/103 [==============================] - 138s 1s/step - loss: 1.7609 - accuracy: 0.5134 - val_loss: 2.3292 - val_accuracy: 0.4511\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 2.12414\n",
            "Epoch 9/10\n",
            "103/103 [==============================] - 137s 1s/step - loss: 1.7679 - accuracy: 0.5223 - val_loss: 2.3836 - val_accuracy: 0.4340\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 2.12414\n",
            "Epoch 10/10\n",
            "103/103 [==============================] - 135s 1s/step - loss: 1.7522 - accuracy: 0.5183 - val_loss: 2.1633 - val_accuracy: 0.4731\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 2.12414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AdPJJn2z8oUR",
        "outputId": "8f969d5c-7839-440f-aa0d-3cf1f5ce9a6d"
      },
      "source": [
        "shutil.move('/content/Landmark_weights_epoch-53_loss-1.1591_val_loss-1.4267.h5', '/content/drive/MyDrive/AIL201/AIP302/flower_weight')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/AIP302/Landmark_weights_epoch-53_loss-1.1591_val_loss-1.4267.h5'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}
